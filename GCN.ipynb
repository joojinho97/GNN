{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cecfcb51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fdd9d03",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-14 09:54:40.818993: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-14 09:54:42.146197: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 46723 MB memory:  -> device: 0, name: NVIDIA RTX A6000, pci bus id: 0000:31:00.0, compute capability: 8.6\n",
      "2023-03-14 09:54:42.147430: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 567 MB memory:  -> device: 1, name: NVIDIA RTX A6000, pci bus id: 0000:b1:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import stellargraph as sg\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import os\n",
    "import stellargraph as sg\n",
    "from stellargraph.mapper import FullBatchNodeGenerator\n",
    "from stellargraph.layer import GCN\n",
    "\n",
    "from tensorflow.keras import layers, optimizers, losses, metrics, Model\n",
    "from sklearn import preprocessing, model_selection\n",
    "from IPython.display import display, HTML\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from spektral.layers import GCNConv, GlobalSumPool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6ee3c464",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]='1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b0ed330",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = sg.datasets.Cora()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f443ca71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "The Cora dataset consists of 2708 scientific publications classified into one of seven classes. The citation network consists of 5429 links. Each publication in the dataset is described by a 0/1-valued word vector indicating the absence/presence of the corresponding word from the dictionary. The dictionary consists of 1433 unique words."
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(HTML(dataset.description))\n",
    "G, node_subjects = dataset.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4ef90d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_subjects, test_subjects = model_selection.train_test_split(\n",
    "    node_subjects, train_size=140, test_size=None, stratify=node_subjects\n",
    ")\n",
    "val_subjects, test_subjects = model_selection.train_test_split(\n",
    "    test_subjects, train_size=500, test_size=None, stratify=test_subjects\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f186ac5d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subject</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Neural_Networks</th>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Genetic_Algorithms</th>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Probabilistic_Methods</th>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Theory</th>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Case_Based</th>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Reinforcement_Learning</th>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Rule_Learning</th>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                        subject\n",
       "Neural_Networks              42\n",
       "Genetic_Algorithms           22\n",
       "Probabilistic_Methods        22\n",
       "Theory                       18\n",
       "Case_Based                   16\n",
       "Reinforcement_Learning       11\n",
       "Rule_Learning                 9"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_subjects.value_counts().to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94ce54b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_encoding = preprocessing.LabelBinarizer()\n",
    "\n",
    "train_targets = target_encoding.fit_transform(train_subjects)\n",
    "val_targets = target_encoding.transform(val_subjects)\n",
    "test_targets = target_encoding.transform(test_subjects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2b38d707",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using GCN (local pooling) filters...\n"
     ]
    }
   ],
   "source": [
    "generator = FullBatchNodeGenerator(G, method=\"gcn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "16f21cd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen = generator.flow(train_subjects.index, train_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c64153aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProteinsGNN(Model):\n",
    " \n",
    "  def __init__(self, n_hidden, n_labels):\n",
    "    super().__init__()\n",
    "    # Define our GCN layer with our n_hidden layers\n",
    "    self.graph_conv = GCNConv(n_hidden)\n",
    "    # Define our global pooling layer\n",
    "    self.pool = GlobalSumPool()\n",
    "    # Define our dropout layer, initialize dropout freq. to .5 (50%)\n",
    "    #self.dropout = Dropout(0.5)\n",
    "    # Define our Dense layer, with softmax activation function\n",
    "    self.dense = tf.keras.layers.Dense(n_labels, 'softmax')\n",
    " \n",
    "# Define class method to call model on input\n",
    "  def call(self, inputs):\n",
    "    out = self.graph_conv(inputs)\n",
    "    out = self.dropout(out)\n",
    "    out = self.pool(out)\n",
    "    out = self.dense(out)\n",
    " \n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d72a5e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ProteinsGNN(32, 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "28059f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "gcn = GCN(\n",
    "    layer_sizes=[16, 16], activations=[\"relu\", \"relu\"], generator=generator, dropout=0.5\n",
    ")\n",
    "x_inp, x_out = gcn.in_out_tensors()\n",
    "\n",
    "x_out\n",
    "predictions = layers.Dense(units=train_targets.shape[1], activation=\"softmax\")(x_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f1754dd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jhjoo/anaconda3/envs/ecg/lib/python3.7/site-packages/keras/optimizer_v2/adam.py:105: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(Adam, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = Model(inputs=x_inp, outputs=predictions)\n",
    "model.compile(\n",
    "    optimizer=optimizers.Adam(lr=0.01),\n",
    "    loss=losses.categorical_crossentropy,\n",
    "    metrics=[\"acc\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6d7b64d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_gen = generator.flow(val_subjects.index, val_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d10d0ad3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "es_callback = EarlyStopping(monitor=\"val_acc\", patience=50, restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9154369f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " input_1 (InputLayer)           [(1, 2708, 1433)]    0           []                               \n",
      "                                                                                                  \n",
      " input_3 (InputLayer)           [(1, None, 2)]       0           []                               \n",
      "                                                                                                  \n",
      " input_4 (InputLayer)           [(1, None)]          0           []                               \n",
      "                                                                                                  \n",
      " dropout (Dropout)              (1, 2708, 1433)      0           ['input_1[0][0]']                \n",
      "                                                                                                  \n",
      " squeezed_sparse_conversion (Sq  (2708, 2708)        0           ['input_3[0][0]',                \n",
      " ueezedSparseConversion)                                          'input_4[0][0]']                \n",
      "                                                                                                  \n",
      " graph_convolution (GraphConvol  (1, None, 16)       22944       ['dropout[0][0]',                \n",
      " ution)                                                           'squeezed_sparse_conversion[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " dropout_1 (Dropout)            (1, None, 16)        0           ['graph_convolution[0][0]']      \n",
      "                                                                                                  \n",
      " graph_convolution_1 (GraphConv  (1, None, 16)       272         ['dropout_1[0][0]',              \n",
      " olution)                                                         'squeezed_sparse_conversion[0][0\n",
      "                                                                 ]']                              \n",
      "                                                                                                  \n",
      " input_2 (InputLayer)           [(1, None)]          0           []                               \n",
      "                                                                                                  \n",
      " gather_indices (GatherIndices)  (1, None, 16)       0           ['graph_convolution_1[0][0]',    \n",
      "                                                                  'input_2[0][0]']                \n",
      "                                                                                                  \n",
      " dense_1 (Dense)                (1, None, 7)         119         ['gather_indices[0][0]']         \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 23,335\n",
      "Trainable params: 23,335\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3bc7f560",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-14 09:54:46.418144: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 - 3s - loss: 1.9535 - acc: 0.1429 - val_loss: 1.9151 - val_acc: 0.1740 - 3s/epoch - 3s/step\n",
      "Epoch 2/200\n",
      "1/1 - 0s - loss: 1.9007 - acc: 0.1857 - val_loss: 1.8764 - val_acc: 0.1800 - 164ms/epoch - 164ms/step\n",
      "Epoch 3/200\n",
      "1/1 - 0s - loss: 1.8448 - acc: 0.2143 - val_loss: 1.8330 - val_acc: 0.1740 - 143ms/epoch - 143ms/step\n",
      "Epoch 4/200\n",
      "1/1 - 0s - loss: 1.7902 - acc: 0.1929 - val_loss: 1.7883 - val_acc: 0.1700 - 151ms/epoch - 151ms/step\n",
      "Epoch 5/200\n",
      "1/1 - 0s - loss: 1.7289 - acc: 0.2286 - val_loss: 1.7419 - val_acc: 0.1860 - 167ms/epoch - 167ms/step\n",
      "Epoch 6/200\n",
      "1/1 - 0s - loss: 1.6544 - acc: 0.2357 - val_loss: 1.6938 - val_acc: 0.2260 - 158ms/epoch - 158ms/step\n",
      "Epoch 7/200\n",
      "1/1 - 0s - loss: 1.6019 - acc: 0.3071 - val_loss: 1.6435 - val_acc: 0.2900 - 177ms/epoch - 177ms/step\n",
      "Epoch 8/200\n",
      "1/1 - 0s - loss: 1.5281 - acc: 0.3286 - val_loss: 1.5924 - val_acc: 0.3800 - 163ms/epoch - 163ms/step\n",
      "Epoch 9/200\n",
      "1/1 - 0s - loss: 1.4793 - acc: 0.4000 - val_loss: 1.5401 - val_acc: 0.4720 - 156ms/epoch - 156ms/step\n",
      "Epoch 10/200\n",
      "1/1 - 0s - loss: 1.3878 - acc: 0.5357 - val_loss: 1.4862 - val_acc: 0.5320 - 156ms/epoch - 156ms/step\n",
      "Epoch 11/200\n",
      "1/1 - 0s - loss: 1.3242 - acc: 0.5500 - val_loss: 1.4309 - val_acc: 0.5780 - 172ms/epoch - 172ms/step\n",
      "Epoch 12/200\n",
      "1/1 - 0s - loss: 1.2471 - acc: 0.6000 - val_loss: 1.3751 - val_acc: 0.5940 - 166ms/epoch - 166ms/step\n",
      "Epoch 13/200\n",
      "1/1 - 0s - loss: 1.1869 - acc: 0.6429 - val_loss: 1.3176 - val_acc: 0.6220 - 159ms/epoch - 159ms/step\n",
      "Epoch 14/200\n",
      "1/1 - 0s - loss: 1.1181 - acc: 0.6643 - val_loss: 1.2587 - val_acc: 0.6480 - 181ms/epoch - 181ms/step\n",
      "Epoch 15/200\n",
      "1/1 - 0s - loss: 1.0463 - acc: 0.7143 - val_loss: 1.1999 - val_acc: 0.6640 - 332ms/epoch - 332ms/step\n",
      "Epoch 16/200\n",
      "1/1 - 0s - loss: 0.9402 - acc: 0.7357 - val_loss: 1.1422 - val_acc: 0.6820 - 162ms/epoch - 162ms/step\n",
      "Epoch 17/200\n",
      "1/1 - 0s - loss: 0.8732 - acc: 0.7571 - val_loss: 1.0877 - val_acc: 0.6900 - 162ms/epoch - 162ms/step\n",
      "Epoch 18/200\n",
      "1/1 - 0s - loss: 0.8397 - acc: 0.8000 - val_loss: 1.0372 - val_acc: 0.6860 - 160ms/epoch - 160ms/step\n",
      "Epoch 19/200\n",
      "1/1 - 0s - loss: 0.7691 - acc: 0.7786 - val_loss: 0.9917 - val_acc: 0.6880 - 150ms/epoch - 150ms/step\n",
      "Epoch 20/200\n",
      "1/1 - 0s - loss: 0.6770 - acc: 0.7929 - val_loss: 0.9499 - val_acc: 0.6960 - 179ms/epoch - 179ms/step\n",
      "Epoch 21/200\n",
      "1/1 - 0s - loss: 0.6815 - acc: 0.8000 - val_loss: 0.9143 - val_acc: 0.7060 - 180ms/epoch - 180ms/step\n",
      "Epoch 22/200\n",
      "1/1 - 0s - loss: 0.5814 - acc: 0.8143 - val_loss: 0.8870 - val_acc: 0.7100 - 161ms/epoch - 161ms/step\n",
      "Epoch 23/200\n",
      "1/1 - 0s - loss: 0.5688 - acc: 0.7857 - val_loss: 0.8645 - val_acc: 0.7180 - 144ms/epoch - 144ms/step\n",
      "Epoch 24/200\n",
      "1/1 - 0s - loss: 0.5456 - acc: 0.8143 - val_loss: 0.8413 - val_acc: 0.7260 - 171ms/epoch - 171ms/step\n",
      "Epoch 25/200\n",
      "1/1 - 0s - loss: 0.4875 - acc: 0.8500 - val_loss: 0.8211 - val_acc: 0.7240 - 152ms/epoch - 152ms/step\n",
      "Epoch 26/200\n",
      "1/1 - 0s - loss: 0.4810 - acc: 0.8071 - val_loss: 0.8054 - val_acc: 0.7280 - 164ms/epoch - 164ms/step\n",
      "Epoch 27/200\n",
      "1/1 - 0s - loss: 0.4513 - acc: 0.8357 - val_loss: 0.7934 - val_acc: 0.7320 - 174ms/epoch - 174ms/step\n",
      "Epoch 28/200\n",
      "1/1 - 0s - loss: 0.3914 - acc: 0.8500 - val_loss: 0.7836 - val_acc: 0.7380 - 156ms/epoch - 156ms/step\n",
      "Epoch 29/200\n",
      "1/1 - 0s - loss: 0.2991 - acc: 0.9143 - val_loss: 0.7769 - val_acc: 0.7480 - 178ms/epoch - 178ms/step\n",
      "Epoch 30/200\n",
      "1/1 - 0s - loss: 0.4014 - acc: 0.8714 - val_loss: 0.7740 - val_acc: 0.7560 - 151ms/epoch - 151ms/step\n",
      "Epoch 31/200\n",
      "1/1 - 0s - loss: 0.2796 - acc: 0.9571 - val_loss: 0.7741 - val_acc: 0.7640 - 158ms/epoch - 158ms/step\n",
      "Epoch 32/200\n",
      "1/1 - 0s - loss: 0.2840 - acc: 0.9286 - val_loss: 0.7754 - val_acc: 0.7680 - 164ms/epoch - 164ms/step\n",
      "Epoch 33/200\n",
      "1/1 - 0s - loss: 0.2267 - acc: 0.9571 - val_loss: 0.7820 - val_acc: 0.7680 - 158ms/epoch - 158ms/step\n",
      "Epoch 34/200\n",
      "1/1 - 0s - loss: 0.2055 - acc: 0.9429 - val_loss: 0.7878 - val_acc: 0.7740 - 178ms/epoch - 178ms/step\n",
      "Epoch 35/200\n",
      "1/1 - 0s - loss: 0.1929 - acc: 0.9429 - val_loss: 0.7932 - val_acc: 0.7780 - 161ms/epoch - 161ms/step\n",
      "Epoch 36/200\n",
      "1/1 - 0s - loss: 0.2120 - acc: 0.9500 - val_loss: 0.7964 - val_acc: 0.7780 - 321ms/epoch - 321ms/step\n",
      "Epoch 37/200\n",
      "1/1 - 0s - loss: 0.1577 - acc: 0.9571 - val_loss: 0.7978 - val_acc: 0.7740 - 152ms/epoch - 152ms/step\n",
      "Epoch 38/200\n",
      "1/1 - 0s - loss: 0.1125 - acc: 0.9714 - val_loss: 0.7986 - val_acc: 0.7800 - 159ms/epoch - 159ms/step\n",
      "Epoch 39/200\n",
      "1/1 - 0s - loss: 0.2046 - acc: 0.9286 - val_loss: 0.8040 - val_acc: 0.7820 - 172ms/epoch - 172ms/step\n",
      "Epoch 40/200\n",
      "1/1 - 0s - loss: 0.1553 - acc: 0.9500 - val_loss: 0.8095 - val_acc: 0.7780 - 137ms/epoch - 137ms/step\n",
      "Epoch 41/200\n",
      "1/1 - 0s - loss: 0.1759 - acc: 0.9643 - val_loss: 0.8147 - val_acc: 0.7780 - 176ms/epoch - 176ms/step\n",
      "Epoch 42/200\n",
      "1/1 - 0s - loss: 0.1308 - acc: 0.9643 - val_loss: 0.8236 - val_acc: 0.7800 - 181ms/epoch - 181ms/step\n",
      "Epoch 43/200\n",
      "1/1 - 0s - loss: 0.1206 - acc: 0.9571 - val_loss: 0.8326 - val_acc: 0.7760 - 176ms/epoch - 176ms/step\n",
      "Epoch 44/200\n",
      "1/1 - 0s - loss: 0.1045 - acc: 0.9643 - val_loss: 0.8391 - val_acc: 0.7800 - 163ms/epoch - 163ms/step\n",
      "Epoch 45/200\n",
      "1/1 - 0s - loss: 0.1048 - acc: 0.9786 - val_loss: 0.8478 - val_acc: 0.7820 - 168ms/epoch - 168ms/step\n",
      "Epoch 46/200\n",
      "1/1 - 0s - loss: 0.1445 - acc: 0.9286 - val_loss: 0.8550 - val_acc: 0.7820 - 145ms/epoch - 145ms/step\n",
      "Epoch 47/200\n",
      "1/1 - 0s - loss: 0.0937 - acc: 0.9571 - val_loss: 0.8670 - val_acc: 0.7840 - 177ms/epoch - 177ms/step\n",
      "Epoch 48/200\n",
      "1/1 - 0s - loss: 0.1256 - acc: 0.9714 - val_loss: 0.8780 - val_acc: 0.7800 - 168ms/epoch - 168ms/step\n",
      "Epoch 49/200\n",
      "1/1 - 0s - loss: 0.0819 - acc: 0.9643 - val_loss: 0.8884 - val_acc: 0.7820 - 159ms/epoch - 159ms/step\n",
      "Epoch 50/200\n",
      "1/1 - 0s - loss: 0.1053 - acc: 0.9643 - val_loss: 0.8997 - val_acc: 0.7840 - 147ms/epoch - 147ms/step\n",
      "Epoch 51/200\n",
      "1/1 - 0s - loss: 0.0902 - acc: 0.9857 - val_loss: 0.9085 - val_acc: 0.7760 - 151ms/epoch - 151ms/step\n",
      "Epoch 52/200\n",
      "1/1 - 0s - loss: 0.1096 - acc: 0.9714 - val_loss: 0.9140 - val_acc: 0.7800 - 153ms/epoch - 153ms/step\n",
      "Epoch 53/200\n",
      "1/1 - 0s - loss: 0.0968 - acc: 0.9643 - val_loss: 0.9190 - val_acc: 0.7780 - 176ms/epoch - 176ms/step\n",
      "Epoch 54/200\n",
      "1/1 - 0s - loss: 0.0909 - acc: 0.9643 - val_loss: 0.9235 - val_acc: 0.7780 - 149ms/epoch - 149ms/step\n",
      "Epoch 55/200\n",
      "1/1 - 0s - loss: 0.1194 - acc: 0.9643 - val_loss: 0.9242 - val_acc: 0.7780 - 165ms/epoch - 165ms/step\n",
      "Epoch 56/200\n",
      "1/1 - 1s - loss: 0.0750 - acc: 0.9714 - val_loss: 0.9227 - val_acc: 0.7820 - 579ms/epoch - 579ms/step\n",
      "Epoch 57/200\n",
      "1/1 - 0s - loss: 0.0896 - acc: 0.9786 - val_loss: 0.9236 - val_acc: 0.7820 - 163ms/epoch - 163ms/step\n",
      "Epoch 58/200\n",
      "1/1 - 0s - loss: 0.0422 - acc: 0.9929 - val_loss: 0.9279 - val_acc: 0.7840 - 169ms/epoch - 169ms/step\n",
      "Epoch 59/200\n",
      "1/1 - 0s - loss: 0.0743 - acc: 0.9929 - val_loss: 0.9316 - val_acc: 0.7860 - 174ms/epoch - 174ms/step\n",
      "Epoch 60/200\n",
      "1/1 - 0s - loss: 0.0503 - acc: 0.9929 - val_loss: 0.9373 - val_acc: 0.7800 - 149ms/epoch - 149ms/step\n",
      "Epoch 61/200\n",
      "1/1 - 0s - loss: 0.1097 - acc: 0.9643 - val_loss: 0.9418 - val_acc: 0.7780 - 166ms/epoch - 166ms/step\n",
      "Epoch 62/200\n",
      "1/1 - 0s - loss: 0.0625 - acc: 0.9786 - val_loss: 0.9481 - val_acc: 0.7780 - 165ms/epoch - 165ms/step\n",
      "Epoch 63/200\n",
      "1/1 - 0s - loss: 0.0648 - acc: 0.9857 - val_loss: 0.9501 - val_acc: 0.7780 - 156ms/epoch - 156ms/step\n",
      "Epoch 64/200\n",
      "1/1 - 0s - loss: 0.0882 - acc: 0.9714 - val_loss: 0.9495 - val_acc: 0.7780 - 152ms/epoch - 152ms/step\n",
      "Epoch 65/200\n",
      "1/1 - 0s - loss: 0.0911 - acc: 0.9643 - val_loss: 0.9448 - val_acc: 0.7820 - 153ms/epoch - 153ms/step\n",
      "Epoch 66/200\n",
      "1/1 - 0s - loss: 0.0675 - acc: 0.9857 - val_loss: 0.9411 - val_acc: 0.7840 - 151ms/epoch - 151ms/step\n",
      "Epoch 67/200\n",
      "1/1 - 0s - loss: 0.0385 - acc: 1.0000 - val_loss: 0.9412 - val_acc: 0.7860 - 360ms/epoch - 360ms/step\n",
      "Epoch 68/200\n",
      "1/1 - 0s - loss: 0.0513 - acc: 0.9929 - val_loss: 0.9455 - val_acc: 0.7860 - 158ms/epoch - 158ms/step\n",
      "Epoch 69/200\n",
      "1/1 - 0s - loss: 0.0796 - acc: 0.9786 - val_loss: 0.9535 - val_acc: 0.7940 - 157ms/epoch - 157ms/step\n",
      "Epoch 70/200\n",
      "1/1 - 0s - loss: 0.0490 - acc: 0.9857 - val_loss: 0.9639 - val_acc: 0.7920 - 157ms/epoch - 157ms/step\n",
      "Epoch 71/200\n",
      "1/1 - 0s - loss: 0.0388 - acc: 0.9929 - val_loss: 0.9768 - val_acc: 0.7900 - 151ms/epoch - 151ms/step\n",
      "Epoch 72/200\n",
      "1/1 - 0s - loss: 0.0118 - acc: 1.0000 - val_loss: 0.9925 - val_acc: 0.7900 - 168ms/epoch - 168ms/step\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 73/200\n",
      "1/1 - 0s - loss: 0.1149 - acc: 0.9643 - val_loss: 0.9964 - val_acc: 0.7920 - 144ms/epoch - 144ms/step\n",
      "Epoch 74/200\n",
      "1/1 - 0s - loss: 0.0554 - acc: 0.9857 - val_loss: 0.9903 - val_acc: 0.7920 - 144ms/epoch - 144ms/step\n",
      "Epoch 75/200\n",
      "1/1 - 0s - loss: 0.0451 - acc: 0.9929 - val_loss: 0.9787 - val_acc: 0.7900 - 157ms/epoch - 157ms/step\n",
      "Epoch 76/200\n",
      "1/1 - 0s - loss: 0.0313 - acc: 0.9929 - val_loss: 0.9691 - val_acc: 0.7920 - 162ms/epoch - 162ms/step\n",
      "Epoch 77/200\n",
      "1/1 - 0s - loss: 0.0509 - acc: 0.9857 - val_loss: 0.9643 - val_acc: 0.7860 - 165ms/epoch - 165ms/step\n",
      "Epoch 78/200\n",
      "1/1 - 0s - loss: 0.0322 - acc: 0.9929 - val_loss: 0.9615 - val_acc: 0.7860 - 150ms/epoch - 150ms/step\n",
      "Epoch 79/200\n",
      "1/1 - 0s - loss: 0.0506 - acc: 0.9786 - val_loss: 0.9622 - val_acc: 0.7880 - 156ms/epoch - 156ms/step\n",
      "Epoch 80/200\n",
      "1/1 - 0s - loss: 0.0521 - acc: 0.9857 - val_loss: 0.9669 - val_acc: 0.7900 - 171ms/epoch - 171ms/step\n",
      "Epoch 81/200\n",
      "1/1 - 0s - loss: 0.0513 - acc: 0.9857 - val_loss: 0.9753 - val_acc: 0.7900 - 171ms/epoch - 171ms/step\n",
      "Epoch 82/200\n",
      "1/1 - 0s - loss: 0.0503 - acc: 0.9857 - val_loss: 0.9848 - val_acc: 0.7840 - 148ms/epoch - 148ms/step\n",
      "Epoch 83/200\n",
      "1/1 - 0s - loss: 0.0444 - acc: 0.9857 - val_loss: 0.9962 - val_acc: 0.7860 - 152ms/epoch - 152ms/step\n",
      "Epoch 84/200\n",
      "1/1 - 0s - loss: 0.0246 - acc: 1.0000 - val_loss: 1.0071 - val_acc: 0.7760 - 152ms/epoch - 152ms/step\n",
      "Epoch 85/200\n",
      "1/1 - 0s - loss: 0.0714 - acc: 0.9714 - val_loss: 1.0231 - val_acc: 0.7740 - 153ms/epoch - 153ms/step\n",
      "Epoch 86/200\n",
      "1/1 - 0s - loss: 0.0712 - acc: 0.9714 - val_loss: 1.0355 - val_acc: 0.7700 - 177ms/epoch - 177ms/step\n",
      "Epoch 87/200\n",
      "1/1 - 0s - loss: 0.0536 - acc: 0.9929 - val_loss: 1.0503 - val_acc: 0.7720 - 156ms/epoch - 156ms/step\n",
      "Epoch 88/200\n",
      "1/1 - 0s - loss: 0.0359 - acc: 0.9929 - val_loss: 1.0664 - val_acc: 0.7680 - 358ms/epoch - 358ms/step\n",
      "Epoch 89/200\n",
      "1/1 - 0s - loss: 0.0207 - acc: 0.9929 - val_loss: 1.0836 - val_acc: 0.7660 - 175ms/epoch - 175ms/step\n",
      "Epoch 90/200\n",
      "1/1 - 0s - loss: 0.0481 - acc: 0.9857 - val_loss: 1.0906 - val_acc: 0.7620 - 144ms/epoch - 144ms/step\n",
      "Epoch 91/200\n",
      "1/1 - 0s - loss: 0.0506 - acc: 0.9857 - val_loss: 1.0868 - val_acc: 0.7600 - 140ms/epoch - 140ms/step\n",
      "Epoch 92/200\n",
      "1/1 - 0s - loss: 0.0304 - acc: 0.9857 - val_loss: 1.0765 - val_acc: 0.7640 - 147ms/epoch - 147ms/step\n",
      "Epoch 93/200\n",
      "1/1 - 0s - loss: 0.0737 - acc: 0.9929 - val_loss: 1.0628 - val_acc: 0.7720 - 149ms/epoch - 149ms/step\n",
      "Epoch 94/200\n",
      "1/1 - 0s - loss: 0.0193 - acc: 1.0000 - val_loss: 1.0565 - val_acc: 0.7740 - 139ms/epoch - 139ms/step\n",
      "Epoch 95/200\n",
      "1/1 - 0s - loss: 0.0415 - acc: 0.9857 - val_loss: 1.0520 - val_acc: 0.7720 - 183ms/epoch - 183ms/step\n",
      "Epoch 96/200\n",
      "1/1 - 0s - loss: 0.0174 - acc: 1.0000 - val_loss: 1.0502 - val_acc: 0.7760 - 159ms/epoch - 159ms/step\n",
      "Epoch 97/200\n",
      "1/1 - 0s - loss: 0.0372 - acc: 0.9929 - val_loss: 1.0478 - val_acc: 0.7760 - 162ms/epoch - 162ms/step\n",
      "Epoch 98/200\n",
      "1/1 - 0s - loss: 0.0417 - acc: 0.9929 - val_loss: 1.0465 - val_acc: 0.7780 - 148ms/epoch - 148ms/step\n",
      "Epoch 99/200\n",
      "1/1 - 0s - loss: 0.0436 - acc: 0.9857 - val_loss: 1.0451 - val_acc: 0.7840 - 367ms/epoch - 367ms/step\n",
      "Epoch 100/200\n",
      "1/1 - 0s - loss: 0.0233 - acc: 0.9857 - val_loss: 1.0449 - val_acc: 0.7800 - 145ms/epoch - 145ms/step\n",
      "Epoch 101/200\n",
      "1/1 - 0s - loss: 0.0629 - acc: 0.9786 - val_loss: 1.0407 - val_acc: 0.7840 - 178ms/epoch - 178ms/step\n",
      "Epoch 102/200\n",
      "1/1 - 0s - loss: 0.0345 - acc: 0.9929 - val_loss: 1.0386 - val_acc: 0.7820 - 167ms/epoch - 167ms/step\n",
      "Epoch 103/200\n",
      "1/1 - 0s - loss: 0.0469 - acc: 0.9857 - val_loss: 1.0364 - val_acc: 0.7820 - 150ms/epoch - 150ms/step\n",
      "Epoch 104/200\n",
      "1/1 - 0s - loss: 0.0365 - acc: 0.9857 - val_loss: 1.0384 - val_acc: 0.7820 - 136ms/epoch - 136ms/step\n",
      "Epoch 105/200\n",
      "1/1 - 0s - loss: 0.0397 - acc: 0.9786 - val_loss: 1.0470 - val_acc: 0.7840 - 159ms/epoch - 159ms/step\n",
      "Epoch 106/200\n",
      "1/1 - 0s - loss: 0.0607 - acc: 0.9857 - val_loss: 1.0534 - val_acc: 0.7860 - 184ms/epoch - 184ms/step\n",
      "Epoch 107/200\n",
      "1/1 - 0s - loss: 0.0337 - acc: 0.9857 - val_loss: 1.0590 - val_acc: 0.7860 - 164ms/epoch - 164ms/step\n",
      "Epoch 108/200\n",
      "1/1 - 0s - loss: 0.0403 - acc: 0.9929 - val_loss: 1.0625 - val_acc: 0.7860 - 168ms/epoch - 168ms/step\n",
      "Epoch 109/200\n",
      "1/1 - 0s - loss: 0.0233 - acc: 0.9929 - val_loss: 1.0649 - val_acc: 0.7840 - 167ms/epoch - 167ms/step\n",
      "Epoch 110/200\n",
      "1/1 - 0s - loss: 0.0416 - acc: 0.9786 - val_loss: 1.0641 - val_acc: 0.7840 - 160ms/epoch - 160ms/step\n",
      "Epoch 111/200\n",
      "1/1 - 0s - loss: 0.0551 - acc: 0.9786 - val_loss: 1.0582 - val_acc: 0.7840 - 167ms/epoch - 167ms/step\n",
      "Epoch 112/200\n",
      "1/1 - 0s - loss: 0.0292 - acc: 0.9857 - val_loss: 1.0543 - val_acc: 0.7860 - 198ms/epoch - 198ms/step\n",
      "Epoch 113/200\n",
      "1/1 - 0s - loss: 0.0191 - acc: 1.0000 - val_loss: 1.0526 - val_acc: 0.7840 - 147ms/epoch - 147ms/step\n",
      "Epoch 114/200\n",
      "1/1 - 0s - loss: 0.0220 - acc: 0.9929 - val_loss: 1.0517 - val_acc: 0.7860 - 144ms/epoch - 144ms/step\n",
      "Epoch 115/200\n",
      "1/1 - 0s - loss: 0.0590 - acc: 0.9714 - val_loss: 1.0530 - val_acc: 0.7800 - 162ms/epoch - 162ms/step\n",
      "Epoch 116/200\n",
      "1/1 - 0s - loss: 0.0430 - acc: 0.9786 - val_loss: 1.0575 - val_acc: 0.7800 - 147ms/epoch - 147ms/step\n",
      "Epoch 117/200\n",
      "1/1 - 0s - loss: 0.0070 - acc: 1.0000 - val_loss: 1.0633 - val_acc: 0.7760 - 155ms/epoch - 155ms/step\n",
      "Epoch 118/200\n",
      "1/1 - 0s - loss: 0.0608 - acc: 0.9786 - val_loss: 1.0675 - val_acc: 0.7760 - 146ms/epoch - 146ms/step\n",
      "Epoch 119/200\n",
      "1/1 - 0s - loss: 0.0456 - acc: 0.9786 - val_loss: 1.0708 - val_acc: 0.7780 - 153ms/epoch - 153ms/step\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_gen,\n",
    "    epochs=200,\n",
    "    validation_data=val_gen,\n",
    "    verbose=2,\n",
    "    shuffle=False,  # this should be False, since shuffling data means shuffling the whole graph\n",
    "    callbacks=[es_callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "979e6afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_gen = generator.flow(test_subjects.index, test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f56c4110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 325ms/step - loss: 0.9974 - acc: 0.7843\n",
      "\n",
      "Test Set Metrics:\n",
      "\tloss: 0.9974\n",
      "\tacc: 0.7843\n"
     ]
    }
   ],
   "source": [
    "test_metrics = model.evaluate(test_gen)\n",
    "print(\"\\nTest Set Metrics:\")\n",
    "for name, val in zip(model.metrics_names, test_metrics):\n",
    "    print(\"\\t{}: {:0.4f}\".format(name, val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bbccf5e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
